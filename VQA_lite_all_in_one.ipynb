{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VQA Lite - All-in-One Notebook\n",
        "\n",
        "Questo notebook contiene tutte le sezioni principali del progetto VQA Lite: configurazione, utilit√†, modello, preparazione dataset (modalit√† leggera), training, valutazione e inferenza. √à pensato per l‚Äôesecuzione locale su MacBook Pro M1 (8GB RAM).\n",
        "\n",
        "Suggerimenti:\n",
        "- Attiva un ambiente virtuale e installa i requisiti prima di eseguire.\n",
        "- Mantieni batch piccoli e limiti nel prepare per non saturare la memoria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installazione/Setup per Colab (auto)\n",
        "import sys, subprocess, os, yaml, torch, random, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    pkgs = ['torch','torchvision','sentence-transformers','tqdm','PyYAML','Pillow','numpy']\n",
        "    subprocess.check_call([sys.executable,'-m','pip','install','-q'] + pkgs)\n",
        "\n",
        "print('Python:', os.popen('python -V').read().strip())\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if hasattr(torch.backends,'mps'):\n",
        "    print('MPS available:', torch.backends.mps.is_available(), 'built:', torch.backends.mps.is_built())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config (Colab-friendly): se mancano i file locali, definisce una config minima in-place\n",
        "if Path('config.yaml').exists():\n",
        "    with open('config.yaml','r') as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "else:\n",
        "    cfg = {\n",
        "        'device': 'auto',\n",
        "        'model': {\n",
        "            'question_dim': 384,\n",
        "            'image_feature_dim': 256,\n",
        "            'attention_hidden_dim': 128,\n",
        "            'dropout': 0.3,\n",
        "        },\n",
        "        'answers': [\"S√¨\",\"No\",\"aeroplano\",\"automobile\",\"uccello\",\"gatto\",\"cervo\",\"cane\",\"rana\",\"cavallo\",\"nave\",\"camion\"],\n",
        "        'categories': {'animale':[2,3,4,5,6,7],'veicolo':[0,1,8,9]},\n",
        "        'training': {'batch_size': 16,'epochs': 3,'learning_rate': 1e-3,'weight_decay':1e-4,'val_split':0.1,'seed':42,'num_workers':0},\n",
        "        'paths': {'model_save_path':'vqa_model_best.pth','train_dataset_path':'data/train_dataset.pkl','test_dataset_path':'data/test_dataset.pkl'},\n",
        "        'embedding_model': 'all-MiniLM-L6-v2'\n",
        "    }\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Drive (opzionale) - Salva/leggi dati e checkpoint da Drive\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "    drive.mount('/content/drive')\n",
        "    base_dir = Path('/content/drive/MyDrive/VQA_lite')\n",
        "    (base_dir / 'data').mkdir(parents=True, exist_ok=True)\n",
        "    cfg['paths']['model_save_path'] = str(base_dir / 'vqa_model_best.pth')\n",
        "    cfg['paths']['train_dataset_path'] = str(base_dir / 'data/train_dataset.pkl')\n",
        "    cfg['paths']['test_dataset_path'] = str(base_dir / 'data/test_dataset.pkl')\n",
        "    # Salva una copia della config aggiornata su Drive\n",
        "    with open(base_dir / 'config_colab.yaml', 'w') as f:\n",
        "        yaml.safe_dump(cfg, f)\n",
        "    print('Drive montato. Base dir:', base_dir)\n",
        "else:\n",
        "    print('Colab non rilevato: salto mount Drive.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utils (compatibili MPS/CPU/CUDA)\n",
        "from torchvision import transforms\n",
        "\n",
        "def get_device(cfg) -> str:\n",
        "    if torch.cuda.is_available():\n",
        "        return 'cuda'\n",
        "    if hasattr(torch.backends,'mps') and torch.backends.mps.is_built() and torch.backends.mps.is_available():\n",
        "        return 'mps'\n",
        "    return 'cpu'\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "def get_image_transform() -> transforms.Compose:\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "DEVICE = get_device(cfg)\n",
        "set_seed(cfg['training'].get('seed', 42))\n",
        "DEVICE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modello (ripreso da src/vqa_model.py)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "class VQANet(nn.Module):\n",
        "    def __init__(self, num_answers, question_dim, image_feature_dim, attention_hidden_dim, dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if name.startswith(\"0\") or name.startswith(\"1\") or name.startswith(\"4\"):\n",
        "                param.requires_grad = False\n",
        "        self.proj = nn.Conv2d(512, image_feature_dim, kernel_size=1)\n",
        "        self.attention_conv = nn.Conv2d(image_feature_dim + question_dim, attention_hidden_dim, 1)\n",
        "        self.attention_fc = nn.Conv2d(attention_hidden_dim, 1, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(image_feature_dim + question_dim, attention_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(attention_hidden_dim, num_answers)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, question_emb, temperature: float = 1.0):\n",
        "        x = self.backbone(image)\n",
        "        img_features = self.proj(x)\n",
        "        B, C, H, W = img_features.shape\n",
        "        question_emb_expanded = question_emb.unsqueeze(-1).unsqueeze(-1).expand(B, -1, H, W)\n",
        "        combined_features = torch.cat([img_features, question_emb_expanded], dim=1)\n",
        "        attn_hidden = torch.tanh(self.attention_conv(combined_features))\n",
        "        logits = self.attention_fc(attn_hidden).view(B, -1)\n",
        "        logits = logits / max(temperature, 1e-6)\n",
        "        attn_weights = F.softmax(logits, dim=1).view(B, 1, H, W)\n",
        "        attended_img_vector = (attn_weights * img_features).sum(dim=[2, 3])\n",
        "        final_combined = torch.cat([attended_img_vector, question_emb], dim=1)\n",
        "        return self.fc(final_combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare Dataset (Colab-optimized, no images in PKL, compressed NPZ)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from torchvision.datasets import CIFAR10\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "DATA_DIR = 'data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "EMBEDDING_MODEL_NAME = cfg['embedding_model']\n",
        "ANSWER_VOCAB = cfg['answers']\n",
        "CATEGORIES = cfg['categories']\n",
        "TRAIN_NPZ = 'data/train_dataset_light.npz'\n",
        "TEST_NPZ = 'data/test_dataset_light.npz'\n",
        "\n",
        "answer_to_idx = {a:i for i,a in enumerate(ANSWER_VOCAB)}\n",
        "cifar10_classes = [\"aeroplano\",\"automobile\",\"uccello\",\"gatto\",\"cervo\",\"cane\",\"rana\",\"cavallo\",\"nave\",\"camion\"]\n",
        "\n",
        "# Build QA triples but store only CIFAR indices, not images\n",
        "\n",
        "def create_indexed_dataset(dataset, description, max_items=2000):\n",
        "    indices = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    label_to_category = {}\n",
        "    for cat, labels in CATEGORIES.items():\n",
        "        for label in labels:\n",
        "            label_to_category[label] = cat\n",
        "    count = 0\n",
        "    for idx, (_, label) in enumerate(tqdm(dataset, desc=f\"Preparazione {description}\")):\n",
        "        true_class_name = cifar10_classes[label]\n",
        "        true_category = label_to_category.get(label)\n",
        "        if true_category:\n",
        "            indices.append(idx)\n",
        "            questions.append(f\"C'√® un {true_category}?\")\n",
        "            answers.append(answer_to_idx[\"S√¨\"])\n",
        "        other_category = 'veicolo' if true_category == 'animale' else 'animale'\n",
        "        indices.append(idx)\n",
        "        questions.append(f\"C'√® un {other_category}?\")\n",
        "        answers.append(answer_to_idx[\"No\"])\n",
        "        indices.append(idx)\n",
        "        questions.append(\"Che oggetto c'√® nell'immagine?\")\n",
        "        answers.append(answer_to_idx[true_class_name])\n",
        "        count += 1\n",
        "        if max_items and count >= max_items:\n",
        "            break\n",
        "    return indices, questions, answers\n",
        "\n",
        "print(\"üîÑ Caricamento CIFAR-10 e modello embedding (CPU)...\")\n",
        "train_set = CIFAR10(root=DATA_DIR, train=True, download=True)\n",
        "test_set = CIFAR10(root=DATA_DIR, train=False, download=True)\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device='cpu')\n",
        "\n",
        "# Limits tuned for Colab Free RAM\n",
        "train_indices, train_questions, train_answers = create_indexed_dataset(train_set, \"Train (light)\", max_items=4000)\n",
        "test_indices, test_questions, test_answers = create_indexed_dataset(test_set, \"Test (light)\", max_items=1000)\n",
        "\n",
        "print(\"‚öôÔ∏è  Calcolo embedding (float16) in batch...\")\n",
        "train_emb = embedding_model.encode(train_questions, convert_to_numpy=True, show_progress_bar=True, batch_size=256, normalize_embeddings=False)\n",
        "test_emb = embedding_model.encode(test_questions, convert_to_numpy=True, show_progress_bar=True, batch_size=256, normalize_embeddings=False)\n",
        "\n",
        "train_emb = train_emb.astype(np.float16, copy=False)\n",
        "test_emb = test_emb.astype(np.float16, copy=False)\n",
        "train_indices = np.asarray(train_indices, dtype=np.int32)\n",
        "test_indices = np.asarray(test_indices, dtype=np.int32)\n",
        "train_answers = np.asarray(train_answers, dtype=np.int16)\n",
        "test_answers = np.asarray(test_answers, dtype=np.int16)\n",
        "\n",
        "print(\"üíæ Salvataggio compresso (.npz)...\")\n",
        "np.savez_compressed(TRAIN_NPZ, indices=train_indices, emb=train_emb, y=train_answers)\n",
        "np.savez_compressed(TEST_NPZ, indices=test_indices, emb=test_emb, y=test_answers)\n",
        "print(\"‚úÖ Dataset light salvato in NPZ compressi.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training (light, Colab-optimized loading)\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "class VQADatasetNPZ(torch.utils.data.Dataset):\n",
        "    def __init__(self, npz_path: str, cifar_split: str = 'train', data_root: str = 'data'):\n",
        "        data = np.load(npz_path)\n",
        "        self.indices = data['indices']\n",
        "        self.emb = data['emb']  # float16\n",
        "        self.y = data['y']\n",
        "        self.transform = get_image_transform()\n",
        "        self.cifar = CIFAR10(root=data_root, train=(cifar_split=='train'), download=True)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, i):\n",
        "        img_idx = int(self.indices[i])\n",
        "        img, _ = self.cifar[img_idx]\n",
        "        image = self.transform(img).float()\n",
        "        q = torch.from_numpy(self.emb[i].astype(np.float32, copy=False))\n",
        "        y = int(self.y[i])\n",
        "        return image, q, y\n",
        "\n",
        "train_ds_full = VQADatasetNPZ('data/train_dataset_light.npz', cifar_split='train', data_root='data')\n",
        "val_split = min(0.2, cfg['training'].get('val_split', 0.1))\n",
        "val_size = max(1, int(len(train_ds_full)*val_split))\n",
        "train_size = len(train_ds_full)-val_size\n",
        "train_ds, val_ds = random_split(train_ds_full, [train_size, val_size])\n",
        "\n",
        "bs = 64 if torch.cuda.is_available() else 8\n",
        "train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "val_loader = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "m = cfg['model']\n",
        "num_answers = len(cfg['answers'])\n",
        "model = VQANet(num_answers, m['question_dim'], m['image_feature_dim'], m['attention_hidden_dim'], dropout=m.get('dropout',0.3)).to(DEVICE)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=cfg['training']['learning_rate'], weight_decay=cfg['training'].get('weight_decay',0.0))\n",
        "\n",
        "use_autocast = torch.cuda.is_available() or (DEVICE=='mps')\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, q, y in loader:\n",
        "            images, q, y = images.to(DEVICE), q.to(DEVICE), torch.tensor(y).to(DEVICE)\n",
        "            if use_autocast and DEVICE!='cpu':\n",
        "                with torch.autocast(device_type='cuda' if DEVICE=='cuda' else 'mps', dtype=torch.float16):\n",
        "                    out = model(images, q)\n",
        "            else:\n",
        "                out = model(images, q)\n",
        "            pred = out.argmax(1)\n",
        "            total += y.size(0)\n",
        "            correct += (pred==y).sum().item()\n",
        "    return 100.0*correct/max(1,total)\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    for images, q, y in train_loader:\n",
        "        images, q, y = images.to(DEVICE), q.to(DEVICE), torch.tensor(y).to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        if use_autocast and DEVICE!='cpu':\n",
        "            with torch.autocast(device_type='cuda' if DEVICE=='cuda' else 'mps', dtype=torch.float16):\n",
        "                out = model(images, q)\n",
        "                loss = criterion(out, y)\n",
        "        else:\n",
        "            out = model(images, q)\n",
        "            loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "    acc = evaluate(model, val_loader)\n",
        "    best_acc = max(best_acc, acc)\n",
        "    print(f\"Epoch 1/1 - val acc: {acc:.2f}%\")\n",
        "\n",
        "print(f\"Best acc: {best_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate (light, NPZ-based)\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "bs = 64 if torch.cuda.is_available() else 8\n",
        "test_ds = VQADatasetNPZ('data/test_dataset_light.npz', cifar_split='test', data_root='data')\n",
        "test_loader = DataLoader(test_ds, batch_size=bs, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "def eval_loader(model, loader):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, q, y in loader:\n",
        "            images, q, y = images.to(DEVICE), q.to(DEVICE), torch.tensor(y).to(DEVICE)\n",
        "            out = model(images, q)\n",
        "            pred = out.argmax(1)\n",
        "            total += y.size(0)\n",
        "            correct += (pred==y).sum().item()\n",
        "    return 100.0*correct/max(1,total)\n",
        "\n",
        "acc_test = eval_loader(model, test_loader)\n",
        "print(f\"Test acc (light): {acc_test:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalizza nomi (rimuove suffisso V2 se presente)\n",
        "if 'train_data_v2' in globals():\n",
        "    train_data = train_data_v2\n",
        "    del train_data_v2\n",
        "if 'test_data_v2' in globals():\n",
        "    test_data = test_data_v2\n",
        "    del test_data_v2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference (Colab: carica una tua immagine opzionale)\n",
        "from PIL import Image\n",
        "\n",
        "answers_vocab = cfg['answers']\n",
        "\n",
        "# Puoi caricare un file da Colab: from google.colab import files; files.upload()\n",
        "img_path = 'data/test_image.jpg'  # Cambia percorso se vuoi\n",
        "question = \"C'√® un cane?\"\n",
        "\n",
        "tr = get_image_transform()\n",
        "img = Image.open(img_path).convert('RGB')\n",
        "img_t = tr(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "emb_model = SentenceTransformer(cfg['embedding_model'], device='cpu')\n",
        "q = emb_model.encode(question, convert_to_tensor=True)\n",
        "if q.dim()==1:\n",
        "    q = q.unsqueeze(0)\n",
        "q = q.to(DEVICE)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(img_t, q)\n",
        "    pred = out.argmax(1).item()\n",
        "\n",
        "print('Domanda:', question)\n",
        "print('Risposta:', answers_vocab[pred])\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
